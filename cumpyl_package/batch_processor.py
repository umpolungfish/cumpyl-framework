import os
import sys
import glob
import threading
import queue
import time
from typing import List, Dict, Any, Callable, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from rich.console import Console
from rich.progress import Progress, TaskID, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
try:
    from .config import ConfigManager
except ImportError:
    from config import ConfigManager


class BatchJob:
    """ğ‘®ğ‘§ğ‘ğ‘®ğ‘¦ğ‘Ÿğ‘§ğ‘¯ğ‘‘ğ‘Ÿ ğ‘© ğ‘•ğ‘¦ğ‘™ğ‘œğ‘©ğ‘¤ ğ‘“ğ‘²ğ‘¤ ğ‘¦ğ‘¯ ğ‘© ğ‘šğ‘¨ğ‘— ğ‘ªğ‘ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯"""
    
    def __init__(self, input_file: str, output_file: str = None, operations: List[Dict[str, Any]] = None):
        self.input_file = input_file
        self.output_file = output_file or self._generate_output_filename(input_file)
        self.operations = operations or []
        self.status = "pending"  # pending, processing, completed, failed
        self.error = None
        self.start_time = None
        self.end_time = None
        self.results = {}
    
    def _generate_output_filename(self, input_file: str) -> str:
        """ğ‘œğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘© ğ‘›ğ‘¦ğ‘“ğ‘·ğ‘¤ğ‘‘ ğ‘¬ğ‘‘ğ‘ğ‘«ğ‘‘ ğ‘“ğ‘²ğ‘¤ğ‘¯ğ‘±ğ‘¥"""
        path = Path(input_file)
        return str(path.parent / f"processed_{path.name}")
    
    def add_operation(self, operation_type: str, **kwargs):
        """ğ‘¨ğ‘› ğ‘© ğ‘¯ğ‘©ğ‘©ğ‘› ğ‘ªğ‘ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯ ğ‘‘ ğ‘ğ‘¦ğ‘• ğ‘¡ğ‘ªğ‘š"""
        self.operations.append({
            'type': operation_type,
            'params': kwargs
        })
    
    def get_duration(self) -> float:
        """ğ‘œğ‘§ğ‘‘ ğ‘ ğ‘›ğ‘˜ğ‘«ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯ ğ‘ ğ‘ğ‘¦ğ‘• ğ‘¡ğ‘ªğ‘š ğ‘¦ğ‘¯ ğ‘•ğ‘§ğ‘’ğ‘©ğ‘¯ğ‘›ğ‘Ÿ"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        elif self.start_time:
            return time.time() - self.start_time
        return 0.0


class BatchProcessor:
    """ğ‘ ğ‘¥ğ‘±ğ‘¯ ğ‘šğ‘¨ğ‘— ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘•ğ‘¦ğ‘™ ğ‘¦ğ‘¯ğ‘¡ğ‘¦ğ‘¯"""
    
    def __init__(self, config: ConfigManager):
        self.config = config
        self.console = Console()
        self.jobs: List[BatchJob] = []
        self.completed_jobs: List[BatchJob] = []
        self.failed_jobs: List[BatchJob] = []
        
        # ğ‘ğ‘®ğ‘§ğ‘› ğ‘ğ‘µğ‘¤ ğ‘’ğ‘ªğ‘¯ğ‘“ğ‘¦ğ‘œ
        self.max_workers = self.config.performance.max_worker_threads if self.config.performance.enable_parallel_processing else 1
        self.progress_queue = queue.Queue()
        
        # ğ‘šğ‘¨ğ‘— ğ‘•ğ‘²ğ‘Ÿ ğ‘¤ğ‘¦ğ‘¥ğ‘¦ğ‘‘ ğ‘“ğ‘¹ ğ‘®ğ‘¦ğ‘¤ğ‘²ğ‘©ğ‘šğ‘¦ğ‘¤ğ‘¦ğ‘‘ğ‘¦
        self.max_batch_size = getattr(self.config.performance, 'max_batch_size', 10)
    
    def add_files(self, file_patterns: List[str], recursive: bool = True) -> int:
        """ğ‘¨ğ‘› ğ‘“ğ‘²ğ‘¤ğ‘Ÿ ğ‘šğ‘±ğ‘•ğ‘‘ ğ‘ªğ‘¯ ğ‘œğ‘¤ğ‘ªğ‘š ğ‘ğ‘¨ğ‘‘ğ‘¼ğ‘¯ğ‘Ÿ"""
        added_count = 0
        
        for pattern in file_patterns:
            if recursive and '**' not in pattern:
                # ğ‘¨ğ‘› ğ‘®ğ‘¦ğ‘’ğ‘»ğ‘•ğ‘¦ğ‘ ğ‘œğ‘¤ğ‘ªğ‘š ğ‘¦ğ‘“ ğ‘¯ğ‘ªğ‘‘ ğ‘©ğ‘¤ğ‘®ğ‘§ğ‘›ğ‘¦ ğ‘¦ğ‘¯ğ‘’ğ‘¤ğ‘¿ğ‘›ğ‘¦ğ‘›
                pattern = os.path.join(os.path.dirname(pattern) or '.', '**', os.path.basename(pattern))
            
            # ğ‘¿ğ‘Ÿ glob ğ‘‘ ğ‘“ğ‘²ğ‘¯ğ‘› ğ‘¥ğ‘¨ğ‘—ğ‘¦ğ‘™ ğ‘“ğ‘²ğ‘¤ğ‘Ÿ
            for filepath in glob.glob(pattern, recursive=recursive):
                if os.path.isfile(filepath):
                    # ğ‘—ğ‘§ğ‘’ ğ‘¦ğ‘“ ğ‘“ğ‘²ğ‘¤ ğ‘•ğ‘²ğ‘Ÿ ğ‘¦ğ‘Ÿ ğ‘©ğ‘’ğ‘•ğ‘§ğ‘ğ‘‘ğ‘©ğ‘šğ‘©ğ‘¤
                    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)
                    if file_size_mb <= self.config.framework.max_file_size_mb:
                        job = BatchJob(os.path.abspath(filepath))
                        self.jobs.append(job)
                        added_count += 1
                    else:
                        if self.config.framework.verbose_logging:
                            self.console.print(f"[yellow]Skipping {filepath}: too large ({file_size_mb:.1f}MB)[/yellow]")
        
        return added_count
    
    def add_directory(self, directory: str, file_extensions: List[str] = None, recursive: bool = True) -> int:
        """ğ‘¨ğ‘› ğ‘·ğ‘¤ ğ‘“ğ‘²ğ‘¤ğ‘Ÿ ğ‘¦ğ‘¯ ğ‘© ğ‘›ğ‘²ğ‘®ğ‘§ğ‘’ğ‘‘ğ‘¼ğ‘¦"""
        if not os.path.isdir(directory):
            raise ValueError(f"Directory does not exist: {directory}")
        
        # ğ‘›ğ‘¦ğ‘“ğ‘·ğ‘¤ğ‘‘ ğ‘§ğ‘’ğ‘•ğ‘§ğ‘’ğ‘¿ğ‘‘ğ‘©ğ‘šğ‘©ğ‘¤ ğ‘¦ğ‘’ğ‘•ğ‘‘ğ‘§ğ‘¯ğ‘–ğ‘©ğ‘¯ğ‘Ÿ
        if file_extensions is None:
            file_extensions = ['.exe', '.dll', '.so', '.bin', '.out', '']  # ğ‘§ğ‘¥ğ‘ğ‘‘ğ‘¦ ğ‘•ğ‘‘ğ‘®ğ‘¦ğ‘™ ğ‘“ğ‘¹ ğ‘“ğ‘²ğ‘¤ğ‘Ÿ ğ‘¢ğ‘¦ğ‘ğ‘¬ğ‘‘ ğ‘¦ğ‘’ğ‘•ğ‘‘ğ‘§ğ‘¯ğ‘–ğ‘©ğ‘¯
        
        patterns = []
        for ext in file_extensions:
            if ext:
                pattern = os.path.join(directory, f"*{ext}")
            else:
                # ğ‘“ğ‘¹ ğ‘“ğ‘²ğ‘¤ğ‘Ÿ ğ‘¢ğ‘¦ğ‘ğ‘¬ğ‘‘ ğ‘¦ğ‘’ğ‘•ğ‘‘ğ‘§ğ‘¯ğ‘–ğ‘©ğ‘¯, ğ‘¢ğ‘° ğ‘¯ğ‘°ğ‘› ğ‘‘ ğ‘—ğ‘§ğ‘’ ğ‘¦ğ‘¯ğ‘›ğ‘¦ğ‘ğ‘¦ğ‘›ğ‘˜ğ‘«ğ‘©ğ‘¤ğ‘¦
                pattern = os.path.join(directory, "*")
            patterns.append(pattern)
        
        return self.add_files(patterns, recursive)
    
    def configure_operation(self, operation_type: str, **kwargs):
        """ğ‘’ğ‘©ğ‘¯ğ‘“ğ‘¦ğ‘œ ğ‘© ğ‘¯ ğ‘ªğ‘ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯ ğ‘‘ ğ‘šğ‘° ğ‘©ğ‘ğ‘¤ğ‘²ğ‘› ğ‘‘ ğ‘·ğ‘¤ ğ‘¡ğ‘ªğ‘šğ‘Ÿ"""
        for job in self.jobs:
            job.add_operation(operation_type, **kwargs)
    
    def _process_single_job(self, job: BatchJob) -> BatchJob:
        """ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘• ğ‘© ğ‘•ğ‘¦ğ‘™ğ‘œğ‘©ğ‘¤ ğ‘¡ğ‘ªğ‘š"""
        try:
            from .cumpyl import BinaryRewriter
        except ImportError:
            from cumpyl import BinaryRewriter
        
        job.status = "processing"
        job.start_time = time.time()
        
        try:
            # ğ‘¦ğ‘¯ğ‘¦ğ‘–ğ‘©ğ‘¤ğ‘²ğ‘Ÿ ğ‘ ğ‘šğ‘²ğ‘¯ğ‘©ğ‘®ğ‘¦ ğ‘®ğ‘°ğ‘®ğ‘²ğ‘‘ğ‘¼
            rewriter = BinaryRewriter(job.input_file, self.config)
            
            # ğ‘¨ğ‘› ğ‘© ğ‘‘ğ‘²ğ‘¥ğ‘¬ğ‘‘ ğ‘“ğ‘¹ ğ‘¤ğ‘´ğ‘›ğ‘¦ğ‘™ ğ‘ğ‘®ğ‘ªğ‘šğ‘¤ğ‘§ğ‘¥ğ‘¨ğ‘‘ğ‘¦ğ‘’ ğ‘“ğ‘²ğ‘¤ğ‘Ÿ
            if not rewriter.load_binary():
                raise Exception(f"Failed to load binary: {job.input_file}")
            
            # ğ‘¤ğ‘´ğ‘› ğ‘ğ‘¤ğ‘³ğ‘œğ‘¦ğ‘¯ğ‘Ÿ ğ‘¦ğ‘“ ğ‘¯ğ‘ªğ‘‘ ğ‘›ğ‘¦ğ‘Ÿğ‘±ğ‘šğ‘©ğ‘¤ğ‘›
            if self.config.plugins.enabled:
                rewriter.load_plugins()
            
            # ğ‘©ğ‘ğ‘¤ğ‘² ğ‘°ğ‘— ğ‘ªğ‘ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯
            for operation in job.operations:
                op_type = operation['type']
                params = operation['params']
                
                if op_type == 'analyze_sections':
                    rewriter.analyze_sections()
                    job.results['section_analysis'] = True
                
                elif op_type == 'plugin_analysis':
                    analysis_results = rewriter.run_plugin_analysis()
                    job.results['plugin_analysis'] = analysis_results
                
                elif op_type == 'encode_section':
                    section_name = params['section_name']
                    encoding = params.get('encoding', 'base64')
                    offset = params.get('offset', 0)
                    length = params.get('length', None)
                    
                    # ğ‘¦ğ‘’ğ‘•ğ‘‘ğ‘®ğ‘¨ğ‘’ğ‘‘ ğ‘¯ ğ‘¦ğ‘¯ğ‘’ğ‘´ğ‘› ğ‘ ğ‘•ğ‘§ğ‘’ğ‘–ğ‘©ğ‘¯
                    try:
                        from .cumpyl import EncodingPlugin
                    except ImportError:
                        from cumpyl import EncodingPlugin
                    encoding_plugin = EncodingPlugin()
                    
                    section_data = rewriter.get_section_data(section_name)
                    if not length or length > len(section_data) - offset:
                        length = len(section_data) - offset
                    
                    encoded_data = encoding_plugin.encode_section_portion(
                        rewriter, section_name, offset, length, encoding
                    )
                    
                    if encoded_data:
                        # ğ‘©ğ‘ğ‘¤ğ‘² ğ‘ ğ‘¦ğ‘¯ğ‘’ğ‘´ğ‘›ğ‘¦ğ‘™ ğ‘‘ ğ‘ ğ‘šğ‘²ğ‘¯ğ‘©ğ‘®ğ‘¦
                        encoded_bytes = encoded_data.encode('utf-8')
                        original_data_portion = section_data[offset:offset+length]
                        
                        if len(encoded_bytes) > len(original_data_portion):
                            encoded_bytes = encoded_bytes[:len(original_data_portion)]
                        elif len(encoded_bytes) < len(original_data_portion):
                            encoded_bytes += b'\x00' * (len(original_data_portion) - len(encoded_bytes))
                        
                        rewriter.modify_section_data(section_name, offset, encoded_bytes)
                        job.results[f'encoded_{section_name}'] = {
                            'encoding': encoding,
                            'offset': offset,
                            'length': length,
                            'success': True
                        }
                
                elif op_type == 'custom':
                    # ğ‘©ğ‘¤ğ‘¬ ğ‘“ğ‘¹ ğ‘’ğ‘³ğ‘•ğ‘‘ğ‘©ğ‘¥ ğ‘ªğ‘ğ‘¼ğ‘±ğ‘–ğ‘©ğ‘¯ğ‘Ÿ
                    custom_func = params.get('function')
                    if custom_func and callable(custom_func):
                        result = custom_func(rewriter, job, params)
                        job.results['custom'] = result
            
            # ğ‘•ğ‘±ğ‘ ğ‘ ğ‘®ğ‘¦ğ‘Ÿğ‘³ğ‘¤ğ‘‘
            if rewriter.save_binary(job.output_file):
                job.status = "completed"
                job.results['output_file'] = job.output_file
            else:
                raise Exception("Failed to save modified binary")
        
        except Exception as e:
            job.status = "failed"
            job.error = str(e)
            if self.config.framework.debug_mode:
                import traceback
                job.error += f"\n{traceback.format_exc()}"
        
        finally:
            job.end_time = time.time()
        
        return job
    
    def process_all(self, progress_callback: Callable = None) -> Dict[str, Any]:
        """ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘• ğ‘·ğ‘¤ ğ‘¡ğ‘ªğ‘šğ‘Ÿ ğ‘¦ğ‘¯ ğ‘ ğ‘’ğ‘¿"""
        if not self.jobs:
            return {
                'completed': 0,
                'failed': 0,
                'total': 0,
                'duration': 0.0
            }
        
        start_time = time.time()
        
        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("({task.completed}/{task.total})"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=self.console
        ) as progress:
            
            task = progress.add_task(
                f"Processing {len(self.jobs)} files...",
                total=len(self.jobs)
            )
            
            if self.max_workers == 1:
                # ğ‘•ğ‘¦ğ‘™ğ‘œğ‘©ğ‘¤-ğ‘ğ‘®ğ‘§ğ‘›ğ‘¦ğ‘› ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘•ğ‘¦ğ‘™
                for job in self.jobs:
                    processed_job = self._process_single_job(job)
                    
                    if processed_job.status == "completed":
                        self.completed_jobs.append(processed_job)
                    else:
                        self.failed_jobs.append(processed_job)
                    
                    progress.advance(task)
                    
                    if progress_callback:
                        progress_callback(processed_job)
            else:
                # ğ‘¥ğ‘©ğ‘¤ğ‘‘ğ‘¦-ğ‘ğ‘®ğ‘§ğ‘›ğ‘¦ğ‘› ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘•ğ‘¦ğ‘™ ğ‘¦ğ‘¯ ğ‘—ğ‘³ğ‘™ğ‘’ğ‘Ÿ ğ‘“ğ‘¹ ğ‘®ğ‘¦ğ‘¤ğ‘²ğ‘©ğ‘šğ‘¦ğ‘¤ğ‘¦ğ‘‘ğ‘¦
                for i in range(0, len(self.jobs), self.max_batch_size):
                    chunk = self.jobs[i:i + self.max_batch_size]
                    chunk_num = (i // self.max_batch_size) + 1
                    total_chunks = (len(self.jobs) + self.max_batch_size - 1) // self.max_batch_size
                    
                    progress.update(task, description=f"Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} files)...")
                    
                    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                        # ğ‘•ğ‘©ğ‘šğ‘¥ğ‘¦ğ‘‘ ğ‘—ğ‘³ğ‘™ğ‘’ ğ‘¡ğ‘ªğ‘šğ‘Ÿ
                        future_to_job = {
                            executor.submit(self._process_single_job, job): job 
                            for job in chunk
                        }
                        
                        # ğ‘’ğ‘©ğ‘¤ğ‘§ğ‘’ğ‘‘ ğ‘®ğ‘¦ğ‘Ÿğ‘³ğ‘¤ğ‘‘ğ‘Ÿ ğ‘“ğ‘¹ ğ‘—ğ‘³ğ‘™ğ‘’
                        chunk_completed = []
                        chunk_failed = []
                        
                        for future in as_completed(future_to_job):
                            processed_job = future.result()
                            
                            if processed_job.status == "completed":
                                self.completed_jobs.append(processed_job)
                                chunk_completed.append(processed_job)
                            else:
                                self.failed_jobs.append(processed_job)
                                chunk_failed.append(processed_job)
                            
                            progress.advance(task)
                            
                            if progress_callback:
                                progress_callback(processed_job)
                        
                        # ğ‘œğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘© ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ ğ‘“ğ‘¹ ğ‘ğ‘¦ğ‘• ğ‘—ğ‘³ğ‘™ğ‘’ ğ‘¦ğ‘“ ğ‘’ğ‘ªğ‘¯ğ‘“ğ‘¦ğ‘œğ‘˜ğ‘¼ğ‘›
                        self._generate_chunk_report(chunk_completed, chunk_failed, chunk_num, total_chunks)
        
        end_time = time.time()
        
        return {
            'completed': len(self.completed_jobs),
            'failed': len(self.failed_jobs),
            'total': len(self.jobs),
            'duration': end_time - start_time,
            'completed_jobs': self.completed_jobs,
            'failed_jobs': self.failed_jobs
        }
    
    def print_summary(self, results: Dict[str, Any]):
        """ğ‘ğ‘®ğ‘¦ğ‘¯ğ‘‘ ğ‘© ğ‘ ğ‘®ğ‘¦ğ‘Ÿğ‘³ğ‘¤ğ‘‘ğ‘Ÿ ğ‘•ğ‘©ğ‘¥ğ‘¼ğ‘¦"""
        self.console.print(Panel("Batch Processing Summary", style="bold cyan"))
        
        # ğ‘´ğ‘ğ‘¼ğ‘·ğ‘¤ ğ‘•ğ‘‘ğ‘©ğ‘‘ğ‘¦ğ‘•ğ‘‘ğ‘¦ğ‘’ğ‘•
        summary_table = Table(show_header=False, box=None)
        summary_table.add_column("Metric", style="bold cyan")
        summary_table.add_column("Value", style="white")
        
        summary_table.add_row("Total Files", str(results['total']))
        summary_table.add_row("Completed", f"[green]{results['completed']}[/green]")
        summary_table.add_row("Failed", f"[red]{results['failed']}[/red]")
        summary_table.add_row("Success Rate", f"{(results['completed'] / results['total'] * 100):.1f}%" if results['total'] > 0 else "0%")
        summary_table.add_row("Total Duration", f"{results['duration']:.2f} seconds")
        summary_table.add_row("Average Time/File", f"{(results['duration'] / results['total']):.2f} seconds" if results['total'] > 0 else "0 seconds")
        
        self.console.print(summary_table)
        
        # ğ‘¦ğ‘“ ğ‘ğ‘» ğ‘¸ ğ‘“ğ‘±ğ‘¤ğ‘› ğ‘¡ğ‘ªğ‘šğ‘Ÿ, ğ‘–ğ‘´ ğ‘ğ‘§ğ‘¥
        if self.failed_jobs:
            self.console.print("\n[red]Failed Jobs:[/red]")
            
            failed_table = Table(show_header=True)
            failed_table.add_column("File", style="cyan")
            failed_table.add_column("Error", style="red")
            failed_table.add_column("Duration", style="yellow")
            
            for job in self.failed_jobs:
                failed_table.add_row(
                    os.path.basename(job.input_file),
                    job.error[:100] + "..." if len(job.error) > 100 else job.error,
                    f"{job.get_duration():.2f}s"
                )
            
            self.console.print(failed_table)
    
    def clear_jobs(self):
        """ğ‘’ğ‘¤ğ‘¦ğ‘¼ ğ‘·ğ‘¤ ğ‘¡ğ‘ªğ‘šğ‘Ÿ (ğ‘§ğ‘’ğ‘•ğ‘ğ‘‘ ğ‘’ğ‘©ğ‘¥ğ‘ğ‘¤ğ‘°ğ‘‘ğ‘¦ğ‘› ğ‘¯ ğ‘“ğ‘±ğ‘¤ğ‘›)"""
        self.jobs.clear()
    
    def get_job_statistics(self) -> Dict[str, Any]:
        """ğ‘œğ‘§ğ‘‘ ğ‘›ğ‘°ğ‘‘ğ‘±ğ‘¤ğ‘› ğ‘ ğ‘•ğ‘‘ğ‘©ğ‘‘ğ‘¦ğ‘•ğ‘‘ğ‘¦ğ‘’ğ‘• ğ‘¦ğ‘¯ ğ‘·ğ‘¤ ğ‘¡ğ‘ªğ‘šğ‘Ÿ"""
        all_jobs = self.completed_jobs + self.failed_jobs
        
        if not all_jobs:
            return {}
        
        durations = [job.get_duration() for job in all_jobs if job.get_duration() > 0]
        
        return {
            'total_jobs': len(all_jobs),
            'completed_jobs': len(self.completed_jobs),
            'failed_jobs': len(self.failed_jobs),
            'success_rate': len(self.completed_jobs) / len(all_jobs) * 100 if all_jobs else 0,
            'avg_duration': sum(durations) / len(durations) if durations else 0,
            'min_duration': min(durations) if durations else 0,
            'max_duration': max(durations) if durations else 0,
            'total_duration': sum(durations) if durations else 0
        }
    
    def _generate_chunk_report(self, chunk_completed: List, chunk_failed: List, chunk_num: int, total_chunks: int):
        """ğ‘œğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘© ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ ğ‘“ğ‘¹ ğ‘© ğ‘•ğ‘¦ğ‘™ğ‘œğ‘©ğ‘¤ ğ‘—ğ‘³ğ‘™ğ‘’ ğ‘ ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘•ğ‘¦ğ‘™"""
        if not self.config.output.split_large_reports:
            return  # ğ‘›ğ‘´ğ‘¯ğ‘‘ ğ‘¡ğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘—ğ‘³ğ‘™ğ‘’ ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ğ‘Ÿ ğ‘¦ğ‘“ ğ‘¯ğ‘ªğ‘‘ ğ‘¦ğ‘¯ğ‘±ğ‘šğ‘©ğ‘¤ğ‘›
        
        try:
            from .reporting import ReportGenerator
        except ImportError:
            from reporting import ReportGenerator
        
        # ğ‘’ğ‘®ğ‘¦ğ‘±ğ‘‘ ğ‘ ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ ğ‘¡ğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ğ‘¼
        report_generator = ReportGenerator(self.config)
        
        # ğ‘’ğ‘®ğ‘¦ğ‘±ğ‘‘ ğ‘šğ‘¨ğ‘— ğ‘®ğ‘¦ğ‘Ÿğ‘³ğ‘¤ğ‘‘ ğ‘›ğ‘¦ğ‘’ğ‘–ğ‘©ğ‘¯ğ‘§ğ‘®ğ‘¦ ğ‘“ğ‘¹ ğ‘ğ‘¦ğ‘• ğ‘—ğ‘³ğ‘™ğ‘’
        chunk_batch_results = {
            'completed': len(chunk_completed),
            'failed': len(chunk_failed), 
            'total': len(chunk_completed) + len(chunk_failed),
            'duration': sum(job.get_duration() for job in chunk_completed + chunk_failed),
            'completed_jobs': chunk_completed,
            'failed_jobs': chunk_failed
        }
        
        # ğ‘œğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘ ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ ğ‘›ğ‘±ğ‘‘ğ‘©
        chunk_report_data = report_generator.create_batch_report(chunk_batch_results)
        
        # ğ‘¨ğ‘› ğ‘—ğ‘³ğ‘™ğ‘’-ğ‘•ğ‘ğ‘§ğ‘•ğ‘¦ğ‘“ğ‘¦ğ‘’ ğ‘¥ğ‘§ğ‘‘ğ‘©ğ‘›ğ‘±ğ‘‘ğ‘©
        chunk_report_data['metadata'].update({
            'chunk_number': chunk_num,
            'total_chunks': total_chunks,
            'chunk_size': len(chunk_completed) + len(chunk_failed),
            'chunk_description': f"Batch chunk {chunk_num} of {total_chunks}",
            'chunk_timestamp': time.strftime('%Y-%m-%d_%H-%M-%S'),
            'chunk_files_processed': [job.input_file for job in chunk_completed + chunk_failed]
        })
        
        # ğ‘’ğ‘®ğ‘¦ğ‘±ğ‘‘ ğ‘ ğ‘ğ‘¦ğ‘¤ ğ‘¯ğ‘±ğ‘¥ ğ‘¢ğ‘¦ğ‘ ğ‘‘ğ‘²ğ‘¥ğ‘•ğ‘‘ğ‘¨ğ‘¥ğ‘ ğ‘¯ ğ‘—ğ‘³ğ‘™ğ‘’ ğ‘¦ğ‘¯ğ‘“ğ‘¼ğ‘¥ğ‘±ğ‘–ğ‘©ğ‘¯
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        output_filename = f"batch_chunk_{chunk_num:03d}_of_{total_chunks:03d}_{timestamp}"
        
        # ğ‘œğ‘§ğ‘¯ğ‘¼ğ‘±ğ‘‘ ğ‘ ğ‘®ğ‘¦ğ‘ğ‘¹ğ‘‘ (ğ‘¿ğ‘Ÿ JSON ğ‘šğ‘² ğ‘›ğ‘¦ğ‘“ğ‘·ğ‘¤ğ‘‘ ğ‘“ğ‘¹ ğ‘“ğ‘¨ğ‘•ğ‘‘ ğ‘ğ‘®ğ‘©ğ‘•ğ‘§ğ‘•ğ‘¦ğ‘™)
        report_generator.generate_report(chunk_report_data, 'json', output_filename)
        
        self.console.print(f"[+] Generated chunk report: {output_filename}.json ({len(chunk_completed)} completed, {len(chunk_failed)} failed)")
        
        return output_filename